{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text = open('C:\\\\Users\\\\richa\\\\wonderland\\\\wonderland.txt').read()\n",
    "raw_text= raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((v,k) for k,v in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:144433\n",
      "Total vocab size:48\n"
     ]
    }
   ],
   "source": [
    "n_chars= len(raw_text)\n",
    "n_vocab =len(chars)\n",
    "print('Total characters:'+str(n_chars))\n",
    "print('Total vocab size:'+str(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:144333\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0,n_chars-seq_len,1):\n",
    "    seq_in= raw_text[i:i+seq_len]\n",
    "    seq_out=raw_text[i+seq_len]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns =len(dataX)\n",
    "print('Total Patterns:'+ str(n_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX,(n_patterns,seq_len,1))\n",
    "X =X/float(n_vocab)\n",
    "y=np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256,input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "144333/144333 [==============================] - 1383s 10ms/step - loss: 2.7868\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.78683, saving model to weights-improvement-01-2.7868-bigger.hdf5\n",
      "Epoch 2/50\n",
      "144333/144333 [==============================] - 1393s 10ms/step - loss: 2.4388\n",
      "\n",
      "Epoch 00002: loss improved from 2.78683 to 2.43877, saving model to weights-improvement-02-2.4388-bigger.hdf5\n",
      "Epoch 3/50\n",
      "144333/144333 [==============================] - 1378s 10ms/step - loss: 2.2316\n",
      "\n",
      "Epoch 00003: loss improved from 2.43877 to 2.23162, saving model to weights-improvement-03-2.2316-bigger.hdf5\n",
      "Epoch 4/50\n",
      "144333/144333 [==============================] - 1359s 9ms/step - loss: 2.0949\n",
      "\n",
      "Epoch 00004: loss improved from 2.23162 to 2.09488, saving model to weights-improvement-04-2.0949-bigger.hdf5\n",
      "Epoch 5/50\n",
      "144333/144333 [==============================] - 1364s 9ms/step - loss: 1.9965\n",
      "\n",
      "Epoch 00005: loss improved from 2.09488 to 1.99648, saving model to weights-improvement-05-1.9965-bigger.hdf5\n",
      "Epoch 6/50\n",
      "144333/144333 [==============================] - 1365s 9ms/step - loss: 1.9193\n",
      "\n",
      "Epoch 00006: loss improved from 1.99648 to 1.91926, saving model to weights-improvement-06-1.9193-bigger.hdf5\n",
      "Epoch 7/50\n",
      "144333/144333 [==============================] - 1373s 10ms/step - loss: 1.8584\n",
      "\n",
      "Epoch 00007: loss improved from 1.91926 to 1.85841, saving model to weights-improvement-07-1.8584-bigger.hdf5\n",
      "Epoch 8/50\n",
      "144333/144333 [==============================] - 1376s 10ms/step - loss: 1.8048\n",
      "\n",
      "Epoch 00008: loss improved from 1.85841 to 1.80475, saving model to weights-improvement-08-1.8048-bigger.hdf5\n",
      "Epoch 9/50\n",
      "144333/144333 [==============================] - 1365s 9ms/step - loss: 1.7581\n",
      "\n",
      "Epoch 00009: loss improved from 1.80475 to 1.75805, saving model to weights-improvement-09-1.7581-bigger.hdf5\n",
      "Epoch 10/50\n",
      "144333/144333 [==============================] - 1388s 10ms/step - loss: 1.7197\n",
      "\n",
      "Epoch 00010: loss improved from 1.75805 to 1.71969, saving model to weights-improvement-10-1.7197-bigger.hdf5\n",
      "Epoch 11/50\n",
      "144333/144333 [==============================] - 1406s 10ms/step - loss: 1.6853\n",
      "\n",
      "Epoch 00011: loss improved from 1.71969 to 1.68529, saving model to weights-improvement-11-1.6853-bigger.hdf5\n",
      "Epoch 12/50\n",
      "144333/144333 [==============================] - 1409s 10ms/step - loss: 1.6476\n",
      "\n",
      "Epoch 00012: loss improved from 1.68529 to 1.64759, saving model to weights-improvement-12-1.6476-bigger.hdf5\n",
      "Epoch 13/50\n",
      "144333/144333 [==============================] - 1416s 10ms/step - loss: 1.6177\n",
      "\n",
      "Epoch 00013: loss improved from 1.64759 to 1.61775, saving model to weights-improvement-13-1.6177-bigger.hdf5\n",
      "Epoch 14/50\n",
      "144333/144333 [==============================] - 1406s 10ms/step - loss: 1.5923\n",
      "\n",
      "Epoch 00014: loss improved from 1.61775 to 1.59226, saving model to weights-improvement-14-1.5923-bigger.hdf5\n",
      "Epoch 15/50\n",
      "144333/144333 [==============================] - 1405s 10ms/step - loss: 1.5647\n",
      "\n",
      "Epoch 00015: loss improved from 1.59226 to 1.56467, saving model to weights-improvement-15-1.5647-bigger.hdf5\n",
      "Epoch 16/50\n",
      "144333/144333 [==============================] - 1406s 10ms/step - loss: 1.5397\n",
      "\n",
      "Epoch 00016: loss improved from 1.56467 to 1.53975, saving model to weights-improvement-16-1.5397-bigger.hdf5\n",
      "Epoch 17/50\n",
      "144333/144333 [==============================] - 1413s 10ms/step - loss: 1.5210\n",
      "\n",
      "Epoch 00017: loss improved from 1.53975 to 1.52102, saving model to weights-improvement-17-1.5210-bigger.hdf5\n",
      "Epoch 18/50\n",
      "144333/144333 [==============================] - 1414s 10ms/step - loss: 1.5014\n",
      "\n",
      "Epoch 00018: loss improved from 1.52102 to 1.50142, saving model to weights-improvement-18-1.5014-bigger.hdf5\n",
      "Epoch 19/50\n",
      "144333/144333 [==============================] - 1413s 10ms/step - loss: 1.4843\n",
      "\n",
      "Epoch 00019: loss improved from 1.50142 to 1.48431, saving model to weights-improvement-19-1.4843-bigger.hdf5\n",
      "Epoch 20/50\n",
      "144333/144333 [==============================] - 1414s 10ms/step - loss: 1.4675\n",
      "\n",
      "Epoch 00020: loss improved from 1.48431 to 1.46746, saving model to weights-improvement-20-1.4675-bigger.hdf5\n",
      "Epoch 21/50\n",
      "144333/144333 [==============================] - 1415s 10ms/step - loss: 1.4545\n",
      "\n",
      "Epoch 00021: loss improved from 1.46746 to 1.45454, saving model to weights-improvement-21-1.4545-bigger.hdf5\n",
      "Epoch 22/50\n",
      "144333/144333 [==============================] - 1411s 10ms/step - loss: 1.4404\n",
      "\n",
      "Epoch 00022: loss improved from 1.45454 to 1.44039, saving model to weights-improvement-22-1.4404-bigger.hdf5\n",
      "Epoch 23/50\n",
      "144333/144333 [==============================] - 1412s 10ms/step - loss: 1.4253\n",
      "\n",
      "Epoch 00023: loss improved from 1.44039 to 1.42534, saving model to weights-improvement-23-1.4253-bigger.hdf5\n",
      "Epoch 24/50\n",
      "144333/144333 [==============================] - 1429s 10ms/step - loss: 1.4125\n",
      "\n",
      "Epoch 00024: loss improved from 1.42534 to 1.41245, saving model to weights-improvement-24-1.4125-bigger.hdf5\n",
      "Epoch 25/50\n",
      "144333/144333 [==============================] - 1483s 10ms/step - loss: 1.4009\n",
      "\n",
      "Epoch 00025: loss improved from 1.41245 to 1.40094, saving model to weights-improvement-25-1.4009-bigger.hdf5\n",
      "Epoch 26/50\n",
      "144333/144333 [==============================] - 1502s 10ms/step - loss: 1.3895\n",
      "\n",
      "Epoch 00026: loss improved from 1.40094 to 1.38948, saving model to weights-improvement-26-1.3895-bigger.hdf5\n",
      "Epoch 27/50\n",
      "144333/144333 [==============================] - 1463s 10ms/step - loss: 1.3799\n",
      "\n",
      "Epoch 00027: loss improved from 1.38948 to 1.37985, saving model to weights-improvement-27-1.3799-bigger.hdf5\n",
      "Epoch 28/50\n",
      "144333/144333 [==============================] - 1454s 10ms/step - loss: 1.3694\n",
      "\n",
      "Epoch 00028: loss improved from 1.37985 to 1.36937, saving model to weights-improvement-28-1.3694-bigger.hdf5\n",
      "Epoch 29/50\n",
      "144333/144333 [==============================] - 1430s 10ms/step - loss: 1.3618\n",
      "\n",
      "Epoch 00029: loss improved from 1.36937 to 1.36176, saving model to weights-improvement-29-1.3618-bigger.hdf5\n",
      "Epoch 30/50\n",
      "144333/144333 [==============================] - 1445s 10ms/step - loss: 1.3538\n",
      "\n",
      "Epoch 00030: loss improved from 1.36176 to 1.35382, saving model to weights-improvement-30-1.3538-bigger.hdf5\n",
      "Epoch 31/50\n",
      "144333/144333 [==============================] - 1441s 10ms/step - loss: 1.3473\n",
      "\n",
      "Epoch 00031: loss improved from 1.35382 to 1.34728, saving model to weights-improvement-31-1.3473-bigger.hdf5\n",
      "Epoch 32/50\n",
      "144333/144333 [==============================] - 1429s 10ms/step - loss: 1.3397\n",
      "\n",
      "Epoch 00032: loss improved from 1.34728 to 1.33971, saving model to weights-improvement-32-1.3397-bigger.hdf5\n",
      "Epoch 33/50\n",
      "144333/144333 [==============================] - 1426s 10ms/step - loss: 1.3299\n",
      "\n",
      "Epoch 00033: loss improved from 1.33971 to 1.32992, saving model to weights-improvement-33-1.3299-bigger.hdf5\n",
      "Epoch 34/50\n",
      "144333/144333 [==============================] - 1434s 10ms/step - loss: 1.3254\n",
      "\n",
      "Epoch 00034: loss improved from 1.32992 to 1.32541, saving model to weights-improvement-34-1.3254-bigger.hdf5\n",
      "Epoch 35/50\n",
      "144333/144333 [==============================] - 1436s 10ms/step - loss: 1.3196\n",
      "\n",
      "Epoch 00035: loss improved from 1.32541 to 1.31956, saving model to weights-improvement-35-1.3196-bigger.hdf5\n",
      "Epoch 36/50\n",
      "144333/144333 [==============================] - 1438s 10ms/step - loss: 1.3152\n",
      "\n",
      "Epoch 00036: loss improved from 1.31956 to 1.31516, saving model to weights-improvement-36-1.3152-bigger.hdf5\n",
      "Epoch 37/50\n",
      "144333/144333 [==============================] - 1447s 10ms/step - loss: 1.3115\n",
      "\n",
      "Epoch 00037: loss improved from 1.31516 to 1.31154, saving model to weights-improvement-37-1.3115-bigger.hdf5\n",
      "Epoch 38/50\n",
      "144333/144333 [==============================] - 1440s 10ms/step - loss: 1.3064\n",
      "\n",
      "Epoch 00038: loss improved from 1.31154 to 1.30642, saving model to weights-improvement-38-1.3064-bigger.hdf5\n",
      "Epoch 39/50\n",
      "144333/144333 [==============================] - 1441s 10ms/step - loss: 1.3038\n",
      "\n",
      "Epoch 00039: loss improved from 1.30642 to 1.30379, saving model to weights-improvement-39-1.3038-bigger.hdf5\n",
      "Epoch 40/50\n",
      "144333/144333 [==============================] - 1426s 10ms/step - loss: 1.3011\n",
      "\n",
      "Epoch 00040: loss improved from 1.30379 to 1.30106, saving model to weights-improvement-40-1.3011-bigger.hdf5\n",
      "Epoch 41/50\n",
      "144333/144333 [==============================] - 1440s 10ms/step - loss: 1.2947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: loss improved from 1.30106 to 1.29467, saving model to weights-improvement-41-1.2947-bigger.hdf5\n",
      "Epoch 42/50\n",
      "144333/144333 [==============================] - 1501s 10ms/step - loss: 1.2933\n",
      "\n",
      "Epoch 00042: loss improved from 1.29467 to 1.29334, saving model to weights-improvement-42-1.2933-bigger.hdf5\n",
      "Epoch 43/50\n",
      "144333/144333 [==============================] - 1449s 10ms/step - loss: 1.2845\n",
      "\n",
      "Epoch 00043: loss improved from 1.29334 to 1.28447, saving model to weights-improvement-43-1.2845-bigger.hdf5\n",
      "Epoch 44/50\n",
      "144333/144333 [==============================] - 1440s 10ms/step - loss: 1.2830\n",
      "\n",
      "Epoch 00044: loss improved from 1.28447 to 1.28299, saving model to weights-improvement-44-1.2830-bigger.hdf5\n",
      "Epoch 45/50\n",
      "144333/144333 [==============================] - 1453s 10ms/step - loss: 1.2831\n",
      "\n",
      "Epoch 00045: loss did not improve\n",
      "Epoch 46/50\n",
      "144333/144333 [==============================] - 1429s 10ms/step - loss: 1.2771\n",
      "\n",
      "Epoch 00046: loss improved from 1.28299 to 1.27710, saving model to weights-improvement-46-1.2771-bigger.hdf5\n",
      "Epoch 47/50\n",
      "144333/144333 [==============================] - 1482s 10ms/step - loss: 1.2777\n",
      "\n",
      "Epoch 00047: loss did not improve\n",
      "Epoch 48/50\n",
      "144333/144333 [==============================] - 1469s 10ms/step - loss: 1.2819\n",
      "\n",
      "Epoch 00048: loss did not improve\n",
      "Epoch 49/50\n",
      "144333/144333 [==============================] - 1463s 10ms/step - loss: 1.2749\n",
      "\n",
      "Epoch 00049: loss improved from 1.27710 to 1.27486, saving model to weights-improvement-49-1.2749-bigger.hdf5\n",
      "Epoch 50/50\n",
      "144333/144333 [==============================] - 1429s 10ms/step - loss: 1.2691\n",
      "\n",
      "Epoch 00050: loss improved from 1.27486 to 1.26912, saving model to weights-improvement-50-1.2691-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ca48b22fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"C://Users//richa//wonderland//weights-improvement-50-1.2691-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" bout two feet high, and was going on shrinking rapidly: she soon found\n",
      "out that the cause of this wa \"\n"
     ]
    }
   ],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s a little boorle oot at all the same with the words and mookes as the book of the shank, and the mouse was a little bottle and surprised to find that it was a little booken was a little boorle of the court, with the droquet was a little saying to the ground had sure the shate of the gourt was a little boorle out of the way the little golden key as she went on, 'i wish it was a little boory that i've got to say the siat cotnd her oig, and the moral of that is was a little boorle of the court, with the dane uith she way she was a little boot to the white rabbit, whth the dromoute and a growne of the goure of the goure of the court, with the dane uith she way she fanlen wery slletiing that is was a little boorle out of the way she was oot at the white rabbit sitee time with the soasess and the book of the gourt, and the three gardeners was a little boorle of the court, with the droquet was a little saying to the ground had sure the shate of the gourt was a little boorle out of the way th\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
