{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text = open('C:\\\\Users\\\\richa\\\\wonderland\\\\wonderland.txt').read()\n",
    "raw_text= raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((v,k) for k,v in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:144433\n",
      "Total vocab size:48\n"
     ]
    }
   ],
   "source": [
    "n_chars= len(raw_text)\n",
    "n_vocab =len(chars)\n",
    "print('Total characters:'+str(n_chars))\n",
    "print('Total vocab size:'+str(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:144333\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "dataX=[]\n",
    "dataY=[]\n",
    "for i in range(0,n_chars-seq_len,1):\n",
    "    seq_in= raw_text[i:i+seq_len]\n",
    "    seq_out=raw_text[i+seq_len]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns =len(dataX)\n",
    "print('Total Patterns:'+ str(n_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = numpy.reshape(dataX,(n_patterns,seq_len,1))\n",
    "X =X/float(n_vocab)\n",
    "y=np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "model.add(LSTM(256,input_shape=(X.shape[1],X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath='C://Users//richa//wonderland//weights-improvement-{epoch:02d}-{loss:0.4f}.hdf5'\n",
    "checkpoint= ModelCheckpoint(filepath,monitor='loss',verbose=1,save_best_only=True,mode='min')\n",
    "callbacks_list=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "144333/144333 [==============================] - 399s 3ms/step - loss: 2.9633\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.96326, saving model to C://Users//richa//wonderland//weights-improvement-01-2.9633.hdf5\n",
      "Epoch 2/20\n",
      "144333/144333 [==============================] - 395s 3ms/step - loss: 2.7548\n",
      "\n",
      "Epoch 00002: loss improved from 2.96326 to 2.75480, saving model to C://Users//richa//wonderland//weights-improvement-02-2.7548.hdf5\n",
      "Epoch 3/20\n",
      "144333/144333 [==============================] - 394s 3ms/step - loss: 2.6465\n",
      "\n",
      "Epoch 00003: loss improved from 2.75480 to 2.64650, saving model to C://Users//richa//wonderland//weights-improvement-03-2.6465.hdf5\n",
      "Epoch 4/20\n",
      "144333/144333 [==============================] - 395s 3ms/step - loss: 2.5725\n",
      "\n",
      "Epoch 00004: loss improved from 2.64650 to 2.57250, saving model to C://Users//richa//wonderland//weights-improvement-04-2.5725.hdf5\n",
      "Epoch 5/20\n",
      "144333/144333 [==============================] - 394s 3ms/step - loss: 2.5105\n",
      "\n",
      "Epoch 00005: loss improved from 2.57250 to 2.51053, saving model to C://Users//richa//wonderland//weights-improvement-05-2.5105.hdf5\n",
      "Epoch 6/20\n",
      "144333/144333 [==============================] - 395s 3ms/step - loss: 2.4552\n",
      "\n",
      "Epoch 00006: loss improved from 2.51053 to 2.45520, saving model to C://Users//richa//wonderland//weights-improvement-06-2.4552.hdf5\n",
      "Epoch 7/20\n",
      "144333/144333 [==============================] - 394s 3ms/step - loss: 2.4034\n",
      "\n",
      "Epoch 00007: loss improved from 2.45520 to 2.40337, saving model to C://Users//richa//wonderland//weights-improvement-07-2.4034.hdf5\n",
      "Epoch 8/20\n",
      "144333/144333 [==============================] - 405s 3ms/step - loss: 2.3546\n",
      "\n",
      "Epoch 00008: loss improved from 2.40337 to 2.35461, saving model to C://Users//richa//wonderland//weights-improvement-08-2.3546.hdf5\n",
      "Epoch 9/20\n",
      "144333/144333 [==============================] - 404s 3ms/step - loss: 2.3063\n",
      "\n",
      "Epoch 00009: loss improved from 2.35461 to 2.30626, saving model to C://Users//richa//wonderland//weights-improvement-09-2.3063.hdf5\n",
      "Epoch 10/20\n",
      "144333/144333 [==============================] - 404s 3ms/step - loss: 2.2635\n",
      "\n",
      "Epoch 00010: loss improved from 2.30626 to 2.26346, saving model to C://Users//richa//wonderland//weights-improvement-10-2.2635.hdf5\n",
      "Epoch 11/20\n",
      "144333/144333 [==============================] - 403s 3ms/step - loss: 2.2197\n",
      "\n",
      "Epoch 00011: loss improved from 2.26346 to 2.21969, saving model to C://Users//richa//wonderland//weights-improvement-11-2.2197.hdf5\n",
      "Epoch 12/20\n",
      "144333/144333 [==============================] - 403s 3ms/step - loss: 2.1808\n",
      "\n",
      "Epoch 00012: loss improved from 2.21969 to 2.18078, saving model to C://Users//richa//wonderland//weights-improvement-12-2.1808.hdf5\n",
      "Epoch 13/20\n",
      "144333/144333 [==============================] - 404s 3ms/step - loss: 2.1408\n",
      "\n",
      "Epoch 00013: loss improved from 2.18078 to 2.14083, saving model to C://Users//richa//wonderland//weights-improvement-13-2.1408.hdf5\n",
      "Epoch 14/20\n",
      "144333/144333 [==============================] - 404s 3ms/step - loss: 2.1051\n",
      "\n",
      "Epoch 00014: loss improved from 2.14083 to 2.10509, saving model to C://Users//richa//wonderland//weights-improvement-14-2.1051.hdf5\n",
      "Epoch 15/20\n",
      "144333/144333 [==============================] - 404s 3ms/step - loss: 2.0695\n",
      "\n",
      "Epoch 00015: loss improved from 2.10509 to 2.06952, saving model to C://Users//richa//wonderland//weights-improvement-15-2.0695.hdf5\n",
      "Epoch 16/20\n",
      "144333/144333 [==============================] - 404s 3ms/step - loss: 2.0338\n",
      "\n",
      "Epoch 00016: loss improved from 2.06952 to 2.03376, saving model to C://Users//richa//wonderland//weights-improvement-16-2.0338.hdf5\n",
      "Epoch 17/20\n",
      "144333/144333 [==============================] - 405s 3ms/step - loss: 2.0045\n",
      "\n",
      "Epoch 00017: loss improved from 2.03376 to 2.00452, saving model to C://Users//richa//wonderland//weights-improvement-17-2.0045.hdf5\n",
      "Epoch 18/20\n",
      "144333/144333 [==============================] - 411s 3ms/step - loss: 1.9757\n",
      "\n",
      "Epoch 00018: loss improved from 2.00452 to 1.97573, saving model to C://Users//richa//wonderland//weights-improvement-18-1.9757.hdf5\n",
      "Epoch 19/20\n",
      "144333/144333 [==============================] - 410s 3ms/step - loss: 1.9477\n",
      "\n",
      "Epoch 00019: loss improved from 1.97573 to 1.94769, saving model to C://Users//richa//wonderland//weights-improvement-19-1.9477.hdf5\n",
      "Epoch 20/20\n",
      "144333/144333 [==============================] - 405s 3ms/step - loss: 1.9181\n",
      "\n",
      "Epoch 00020: loss improved from 1.94769 to 1.91811, saving model to C://Users//richa//wonderland//weights-improvement-20-1.9181.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19cbad3d240>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,epochs=20,batch_size=128,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('C:\\\\Users\\\\richa\\\\wonderland\\\\weights-improvement-20-1.9181.hdf5')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ough; don't be particular--here, bill! catch hold of this\n",
      "rope--will the roof bear?--mind that loose \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng an the coulors ' and the mone turtle caae so the soeee \n",
      "and whnt hare aelan to thi the soreo. \n",
      "'ie you sain to bine'' said the macci hareil aedin, and the pooe lare and the wonle rabbit so the soeee an once to the soeee  and whn gar aelun the was so ton the tooe of the court, and whet saed doon and toiee an in sas on the tooe of the court, \n",
      "and the sooee had so benn to the thit oi the court, and whs gad feen wo the thit was oo tor oo tot oo the wint with she was of the tonee aalkt in the care a little sa thre the was of the toeee sae at ati the rase tas so the tooe of the sooes.\n",
      "\n",
      "'thet se tou doen ' said the monke, and the sonee hare and toene thie thth the garter an on her fend and the calk an  the harter aadin to the tooe, and was toenk flrs and toted and the was oo ten the was so tan the wist on the cank.\n",
      "\n",
      "'  '                                                                                                                                                                            \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
